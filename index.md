## Organizers
<p style="text-align: center;"> Sean Fanello, Christoph Rhemann, Graham Fyffe,  Jonathan Taylor, Sofien Bouaziz, Paul Debevec, Shahram Izadi
</p>
<p style="text-align: center;"> <b> Google </b> </p>

## Description
Volumetric (4D) performance capture is fundamental for AR/VR content generation.  Designing a volumetric capture pipeline involves developing a full computer vision stack: from novel sensors to efficient volumetric reconstruction algorithms, compression and rendering. To this end, we leverage a combination of active sensors with traditional photometric stereo methods. As a result, we have developed an unique pipeline for reconstruction, tracking and texturing of humans in 4D.

In this tutorial we will walk the attendee through the ins and outs of building such a system from the ground up.

First, we will consider the hardware design choices for cameras, sensors, lighting, and depth estimation algorithms. 

In the second part we will cover reconstruction and tracking techniques for people. We will review state of the art algorithms to generate high quality meshes and volumetric representations.

In the third part we will focus on photometric stereo, texturing, relightability and rendering: we will detail the state of the art algorithms together with our choices.

Finally we will discuss some applications and capabilities that 3D capture technologies enable. We will put emphasis on virtual and augmented reality scenarios and highlight recent trends in machine learning that aim at replacing traditional graphics pipelines.

## When and Where
June 16th -  Long Beach Convention Center in Long Beach, Room 201A

## Program

<table style="width:100%">
  <tr>
    <th><div align="center"> Time</div> </th>
    <th><div align="center"> Title</div> </th> 
    <th><div align="center"> Speaker</div> </th>
  </tr>
  <tr>
    <td><div align="center"> 9:00 - 9:30 </div> </td>
    <td><div align="center"> [Keynote] Digitizing Humans </div> </td> 
    <td><div align="center"> Shahram Izadi<br/> Google </div> </td> 
  </tr>
  <tr>
    <td></td>
    <td><div align="center"> <b> Morning Session: <br/>Volumetric Capture of Humans </b> </div> </td> 
    <td></td>
  </tr>
  <tr>
    <td><div align="center"> 9:30 - 10:00 </div> </td>
    <td><div align="center"> High Quality Depth Sensors for Volumetric Capture </div> </td> 
    <td><div align="center"> Adarsh Kowdle<br/> Google </div> </td> 
  </tr>  
  <tr>
    <td><div align="center"> 10:00 - 10:30 </div> </td>
    <td><div align="center"> Depth Estimation via Triangulation: Basics, Challenges and Algorithms </div> </td> 
    <td><div align="center"> Sergio Orts Escolano <br/> Google </div> </td> 
  </tr>      
  <tr>
    <td><div align="center"> 10:30 - 10:45 </div> </td>
    <td><div align="center"> Coffee Break </div> </td> 
    <td></td> 
  </tr>      
  <tr>
    <td><div align="center"> 10:45 - 11:15 </div> </td>
    <td><div align="center"> Depth Estimation in the Age of Deep Learning </div> </td> 
    <td><div align="center"> Yinda Zhang, Christian Haene, Sameh Khamis <br/> Google </div> </td> 
  </tr>      
  <tr>
    <td><div align="center"> 11:15 - 11:50 </div> </td>
    <td><div align="center"> Multiview Human Pose Estimation via Deep Learning </div> </td> 
    <td><div align="center"> Anastasia Tkach <br/> Google </div> </td> 
  </tr> 
   <tr>
    <td><div align="center"> 11:50 - 12:30 </div> </td>
    <td><div align="center"> Detect, Reconstruct, Track and Parameterize Humans</div> </td> 
    <td><div align="center"> Sean Fanello, Kaiwen Guo <br/> Google </div> </td> 
  </tr>     
  <tr>
    <td><div align="center"> 12:30 - 13:30 </div> </td>
    <td><div align="center"> Lunch Break </div> </td> 
    <td></td> 
  </tr>   
  <tr>
    <td></td>
    <td><div align="center"> <b> Afternoon Session: <br/>Light Stage for High Quality Renderings
 </b> </div> </td> 
    <td></td>
  </tr>  
   <tr>
    <td><div align="center"> 13:30 - 14:00 </div> </td>
    <td><div align="center"> [Keynote] A Light Stage for (almost) Every Application </div> </td> 
    <td><div align="center"> Paul Debevec <br/> Google </div> </td> 
  </tr>    
  <tr>
    <td><div align="center"> 14:00 - 14:25 </div> </td>
    <td><div align="center"> The Lightstage Hardware </div> </td> 
    <td><div align="center"> Xueming Yu <br/> Google </div> </td> 
  </tr>    
  <tr>
    <td><div align="center"> 14:25 - 15:00 </div> </td>
    <td><div align="center"> Relighting Still Images and Videos </div> </td> 
    <td><div align="center"> Graham Fyffe <br/> Google </div> </td> 
  </tr>  
    <tr>
    <td><div align="center"> 15:00 - 15:20 </div> </td>
    <td><div align="center"> Relightable Volumetric Capture of Humans </div> </td> 
    <td><div align="center"> Graham Fyffe <br/> Google </div> </td> 
  </tr> 
    <tr>
    <td><div align="center"> 15:20 - 15:40 </div> </td>
    <td><div align="center"> Single Image Portrait Relighting </div> </td> 
    <td><div align="center"> Tiancheng Sun <br/> UCSD </div> </td> 
  </tr>  
    <td><div align="center"> 15:40 - 16:00 </div> </td>
    <td><div align="center"> Coffee Break </div> </td> 
    <td></td> 
  <tr>
    <td><div align="center"> 16:00 - 16:20 </div> </td>
    <td><div align="center"> Neural rendering for Performance Capture </div> </td> 
    <td><div align="center"> Rohit Pandey <br/> Google </div> </td> 
    <td></td> 
  </tr>   
      <tr>
    <td><div align="center"> 16:20 - 16:40 </div> </td>
    <td><div align="center"> Neural Rerendering in the Wild </div> </td> 
    <td><div align="center"> Moustafa Meshry <br/> University of Maryland </div> </td> 
  </tr> 
  <tr>
    <td><div align="center"> 16:40 - 17:00 </div> </td>
    <td><div align="center"> Deep Reflectance Fields </div> </td> 
    <td><div align="center"> Abhimitra Meka <br/> MPI </div> </td> 
  </tr>      
    
</table>

Please contact [Shahram Izadi](mailto:shahrami@google.com) or [Sean Fanello](mailto:seanfa@google.com) if you have any questions.
